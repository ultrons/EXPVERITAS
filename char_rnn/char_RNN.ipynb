{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 5283795 characters and 80 unique.\n"
     ]
    }
   ],
   "source": [
    "# Reading the data\n",
    "data = open('sample_input.txt').read()\n",
    "\n",
    "# Vocabulary business\n",
    "\n",
    "# 1. Create a list of unique characters\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_chars = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "x=np.zeros(len(data))\n",
    "for i, c in enumerate(data):\n",
    "    x[i]=char_to_ix[c]\n",
    "\n",
    "data=x\n",
    "\n",
    "print(\"The dataset has %d characters and %d unique.\" %(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defining hyperparameter tuple and setting hyper parameters\n",
    "hparams = namedtuple('hyper_parameters', \n",
    "                     'hidden_size, seq_length, learning_rate,'\n",
    "                     'batch_size, vocab_size,'\n",
    "                    'num_epochs, num_layers, keep_prob')\n",
    "\n",
    "\n",
    "# Using the hyper parameters also used by:\n",
    "# Martin Gorner\n",
    "#https://github.com/martin-gorner/tensorflow-rnn-shakespeare/blob/master/rnn_train.py\n",
    "\n",
    "hps = hparams(hidden_size=512,\n",
    "              seq_length=30,\n",
    "              learning_rate=1e-3,\n",
    "              batch_size=200,\n",
    "              vocab_size=vocab_size,\n",
    "              num_epochs=50,\n",
    "             num_layers=3,\n",
    "             keep_prob=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class babble(object):\n",
    "    def __init__(self, hps, mode, debug=True):\n",
    "        self.hps=hps\n",
    "        self.mode=mode\n",
    "        self.batch_pointer=None\n",
    "    \n",
    "        \n",
    "    def buildGraph(self, variant=\"fixed_length\"):\n",
    "        # Extracting structural specifics from HPS\n",
    "        D = self.hps.vocab_size\n",
    "        H = self.hps.hidden_size\n",
    "        N = self.hps.batch_size\n",
    "        T = self.hps.seq_length\n",
    "\n",
    "        # Placeholder\n",
    "        with tf.name_scope(\"PlaceHolders\"):\n",
    "            self.X = tf.placeholder(tf.int32, [None, None], \"Inputs\")\n",
    "            self.Y = tf.placeholder(tf.int32, [None, None], \"Expected_Output\")\n",
    "            #is_training = tf.placeholder(tf.bool)\n",
    "            self.h0 = tf.placeholder(tf.float32, [None, self.hps.num_layers*self.hps.hidden_size], \"initial_hidden_state\")\n",
    "        \n",
    "        # No projection to embedding is performed in this experiment\n",
    "        # Inputs are simply translated to one hot\n",
    "        inputs = tf.one_hot(self.X,depth=self.hps.vocab_size)\n",
    "            \n",
    "        #with tf.name_scope(\"batch_norm\"):\n",
    "        #    inputs = tf.layers.batch_normalization(inputs)\n",
    "        cell = tf.contrib.rnn.GRUCell(H)\n",
    "        \n",
    "    \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.hps.keep_prob)\n",
    "        \n",
    "\n",
    "        multi_cell = tf.contrib.rnn.MultiRNNCell([cell]*self.hps.num_layers, state_is_tuple=False)\n",
    "\n",
    "        input_shape = tf.shape(self.X)\n",
    "        \n",
    "        #states = multi_cell.zero_state(self.hps.batch_size, tf.float32)\n",
    "        self.zerostate = multi_cell.zero_state(input_shape[0], dtype=tf.float32) \n",
    "        outputs, self.hidden_state = tf.nn.dynamic_rnn(\n",
    "                                         cell=multi_cell,\n",
    "                                         dtype=tf.float32,\n",
    "                                         inputs=inputs,\n",
    "                                         initial_state=self.h0\n",
    "                     \n",
    "        )\n",
    "\n",
    "        \n",
    "        # Add an operation to update the train states with the last state tensors.\n",
    "        #update_op = get_state_update_op(states, last_states)\n",
    "\n",
    "\n",
    "    \n",
    "        #with tf.control_dependencies([states.assign(last_states)]):\n",
    "        #    outputs = outputs\n",
    "\n",
    "             \n",
    "        with tf.name_scope(\"Dense_Output_Layer\"):\n",
    "            outputs=tf.reshape(outputs, [-1, H])\n",
    "            scores=tf.layers.dense(outputs, D)\n",
    "        tf.summary.histogram('scores', scores)\n",
    "        self.scores=scores\n",
    "        return self.scores\n",
    "      \n",
    "        \n",
    "    def trainStep(self, scores):\n",
    "        # Extracting structural specifics from HPS\n",
    "        D = self.hps.vocab_size\n",
    "        H = self.hps.hidden_size\n",
    "        N = self.hps.batch_size\n",
    "        T = self.hps.seq_length\n",
    "        \n",
    "      \n",
    "        \n",
    "        # Define the additional part of the network Used for training\n",
    "        # Loss and Optimizer\n",
    "        y_int=tf.reshape(self.Y, [-1])\n",
    "        with tf.name_scope(\"COST\"):\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.one_hot(y_int,self.hps.vocab_size),\n",
    "                logits=scores,\n",
    "                name=\"softMaxCrossEntropy\"\n",
    "            )\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        tf.summary.scalar('loss_', loss)\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Predictions\"):\n",
    "            predictions = tf.cast(tf.argmax(scores, axis=-1, name=\"predictions\"), tf.int32)\n",
    "        #accuracy=tf.reduce_mean(tf.cast(tf.equal(predictions, self.Y), tf.float32))\n",
    "                                         \n",
    "        solver = tf.train.AdamOptimizer(self.hps.learning_rate)\n",
    "        #solver = tf.train.GradientDescentOptimizer(hps.learning_rate)\n",
    "        #solver = tf.train.MomentumOptimizer(hps.learning_rate, 0.9)\n",
    "        \n",
    "        tvars  = tf.trainable_variables()\n",
    "        gs_int = tf.gradients(loss, tvars)\n",
    "        grads = list(zip(gs_int, tf.trainable_variables()))\n",
    "        gs, _  = tf.clip_by_global_norm(gs_int, 3.0)\n",
    "        # for grad, var in grads:\n",
    "        #     tf.summary.histogram(var.name + '/gradient', grad)\n",
    "        #tf.summary.histogram('gradients', gs)\n",
    "        train_step = solver.apply_gradients(zip(gs,tvars), global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        #train_step = solver.apply_gradients(grads, global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        summary = tf.summary.merge_all()\n",
    "        return train_step, loss, summary, self.zerostate\n",
    "   \n",
    "    def train(self,sess, data,train_ops, writer=None, print_every=10):\n",
    "        itr=0\n",
    "        num_batches=data.shape[0]//self.hps.batch_size//self.hps.seq_length\n",
    "        #print(num_batches)\n",
    "        #istate = sess.run(self.zerostate) # initial zero input state (a tuple)\n",
    "        for e in tqdm(list(range(hps.num_epochs)), desc='epoch'):\n",
    "            \n",
    "            total_correct=0\n",
    "            #print(\"Reset\", total_correct)\n",
    "            for i in range(num_batches):\n",
    "                itr+=1\n",
    "                x,y = self.getNextBatch(data)\n",
    "                feed_dict={self.X: x, self.Y:y, self.h0:np.zeros((self.hps.batch_size, self.hps.hidden_size*self.hps.num_layers))}\n",
    "                _, loss, summary,_ = sess.run(train_ops, feed_dict=feed_dict)\n",
    "                #if (i % print_every == 0):\n",
    "                #print(\"Loss, Iter %d: %f\" %(i, loss))\n",
    "                if writer is not None:\n",
    "                    writer.add_summary(summary)\n",
    "            #Sample after every 10 epochs to see how we are doing\n",
    "            if (itr%200 == 0): \n",
    "                self.sample(sess,self.scores)\n",
    "\n",
    "        \n",
    "    \n",
    "    def sample(self, sess, scores, \n",
    "               seed=\"I have something to say\",\n",
    "               length=200, beam_width=5):\n",
    "        #istate = sess.run(self.zerostate) # initial zero input state (a tuple)\n",
    "        pd = tf.nn.softmax(scores)\n",
    "        x= [char_to_ix[i] for i in seed]\n",
    "        x=np.asarray(x).reshape(1,-1)\n",
    "        y=np.zeros_like(x)\n",
    "        feed_dict={self.X:x, self.Y:y, self.h0:np.zeros((1,self.hps.hidden_size*self.hps.num_layers))}\n",
    "        pred_str=[]\n",
    "        #print(x.shape, y.shape)\n",
    "        for i in range(length):\n",
    "            p,h0=sess.run([pd,self.hidden_state], feed_dict=feed_dict)\n",
    "            feed_dict[self.h0]=h0\n",
    "            p=p[-1]\n",
    "            #print(p.shape)\n",
    "            ix = np.random.choice(range(self.hps.vocab_size), p=p.ravel())\n",
    "            #ix=np.argmax(p.ravel())\n",
    "            p.reshape(1,-1)\n",
    "            feed_dict[self.X]=ix*np.ones((1,1))\n",
    "            pred_str.append(ix_to_chars[ix])\n",
    "        txt=''.join(pred_str)\n",
    "        #print(len(pred_str))\n",
    "        print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # data is expected to be numpy array of indices\n",
    "    def getNextBatch(self, data):\n",
    "        #print(data.shape[0])\n",
    "        if self.batch_pointer is None:\n",
    "            segment=data.shape[0]//self.hps.batch_size\n",
    "            self.batch_pointer = np.array([offset*segment for offset in range(self.hps.batch_size)])\n",
    "        else:\n",
    "            self.batch_pointer += 1\n",
    "            self.batch_pointer %= data.shape[0]\n",
    "        \n",
    "        \n",
    "        x=np.zeros((self.hps.batch_size, self.hps.seq_length))\n",
    "        y=np.zeros((self.hps.batch_size, self.hps.seq_length))\n",
    "        \n",
    "        indices=self.batch_pointer\n",
    "        \n",
    "        for i in range(self.hps.seq_length):\n",
    "            x[:,i]=np.take(data,indices, mode='wrap')\n",
    "            y[:,i]=np.take(data,indices+1, mode='wrap')\n",
    "            indices+=1\n",
    "            \n",
    "        return x,y\n",
    "    \n",
    "#def batch2String(x):\n",
    "        \n",
    "        \n",
    "                                \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 30)\n"
     ]
    }
   ],
   "source": [
    "model=babble (hps,'train')\n",
    "sample=model.getNextBatch(data)\n",
    "print(sample[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  18%|█▊        | 9/50 [09:39<43:59, 64.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  'doth trust.\n",
      "\tRomad is the gleans would die aforth, but an eunchaled grave do you contract?\n",
      "ICAS\t|  so fougies aloles.\n",
      "\n",
      "LUCILIUS\tNothance I say:, despite of you, sir, let me give: yet they could so p \n",
      "----\n",
      "----\n",
      " , thou easure falls in the name: o life!\n",
      "\n",
      "\t[They feed our Gracieness. Poor EGO]  So here, it is my lord, a day a wife.\n",
      "\n",
      "CLEOPETRA\tWhat had\n",
      "\tthis: go, sir: you are ye before Ajax\n",
      "\tlove no preizon in pr \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  28%|██▊       | 14/50 [15:02<38:42, 64.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "  his business, I will be forsidea.\n",
      "\tWhat haviling so rude is not most welcome; I will say there, heat me. I take thy gallic countenance and an understanding by compulsion and disgrace!\n",
      "\tWhat laby his  \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  38%|███▊      | 19/50 [20:24<33:17, 64.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "  pieces' Double blessing up\n",
      "\tResolved in the orsligmans on my head between she scorn'd me with a rare escage. Adieu; quick, know: 'hold her Diack: he'll do it for this satisfactious murders and seeany \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  48%|████▊     | 24/50 [25:50<28:07, 64.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      " \n",
      "\tyet, haith upray with me: 'tis turn'd this wind:\n",
      "\tNo, good my lord,\n",
      "\tIf not your hemless tricks again. Happy venier! This kiss's palace?\n",
      "\n",
      "IAGO\tO, elder arpoing, how hardhy her see shalt call thy fit \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  58%|█████▊    | 29/50 [31:13<22:35, 64.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      " , for let some meon scurvy counterfeit.\n",
      "\n",
      "AJMARDONA\tJuda, my lord:\n",
      "\tI must appearing, sir, full of reason: but he is untraded in all trick of sorrows too, if ford, not night;\n",
      "\tThe children fled, where  \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  68%|██████▊   | 34/50 [36:40<17:26, 65.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "  Jucih and this?\n",
      "\n",
      "HAMLET\tSee't shaking you with his short and more flyending sickness of deny,\n",
      "\tWhether he you give heavy letters be within that?\n",
      "\n",
      "QUEEN GERTRUDE\tDying of hollow Coriolanus.\n",
      "\n",
      "ARVIRAGUS \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  78%|███████▊  | 39/50 [42:02<11:50, 64.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      " .\n",
      "\n",
      "MENAS\tSeen\tThat, of Hero, but adamed with the armour of your lordship.\n",
      "\n",
      "SHALLOW\tWhy, he kill'd them to him: Longanltit the continent: philosophie she is not our dull care\n",
      "\tIf yet cow a boom knighti \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  88%|████████▊ | 44/50 [47:25<06:26, 64.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      " ,\n",
      "\tLet is but blast; the winds go then takes the sooner and foil, I am bertres openly.\n",
      "\n",
      "PAROLLES\tHe doth one.\n",
      "\n",
      "\t[Enter MARK ANTONAN, MENENIUS, and attendants]\n",
      "\n",
      "KING HENRY VIII\t[Within]  My lord, as he \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  98%|█████████▊| 49/50 [52:48<01:04, 64.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "  'Welro is thine honest,\n",
      "\tWe\n",
      "\thad pending till the blood, and chousiddund: and this is not that there's the bottous body of the borrow'd bears: good yokhip,' your royal lady laughed to hold his gait.\n",
      " \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch: 100%|██████████| 50/50 [53:52<00:00, 64.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "scores=model.buildGraph()\n",
    "train_ops=model.trainStep(scores)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)\n",
    "summaries=tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\n",
    "            os.path.join('./tf_logs', time.strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model.train(sess, data, train_ops,writer)\n",
    "    #model.sample(sess, scores) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " , with whipt in Rome regards vightousse.\n",
      "\n",
      "PANDARUS\tTherefore, do you right: after more thing, ha,'--this suages and smuth books;\n",
      "\tHave I find it to seek him once to thine eyes, that have respected wit \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "model.sample(sess, scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.sample(sess, scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
