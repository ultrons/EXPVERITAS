{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Create a basic sequence training and sampling mechanism using tensorflow. This work is like multitude of similar works emulating https://gist.github.com/karpathy/d4dee566867f8291f086 (karpathy/min-char-rnn.py). Data pre-processing is directly borrowed from aforesaid post. \n",
    "\n",
    "Dataset is bigger (~5X) than that used in the original experiment (included in this repository.)\n",
    "\n",
    "Only cross-entropy is tracked here. Other language model specific parameters like Prelexity of the model is not tracked here.\n",
    "\n",
    "....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 5283795 characters and 80 unique.\n"
     ]
    }
   ],
   "source": [
    "# Reading the data\n",
    "data = open('sample_input.txt').read()\n",
    "\n",
    "# Vocabulary business\n",
    "\n",
    "# 1. Create a list of unique characters\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_chars = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "x=np.zeros(len(data))\n",
    "for i, c in enumerate(data):\n",
    "    x[i]=char_to_ix[c]\n",
    "\n",
    "data=x\n",
    "\n",
    "print(\"The dataset has %d characters and %d unique.\" %(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Defining hyperparameter tuple and setting hyper parameters\n",
    "hparams = namedtuple('hyper_parameters', \n",
    "                     'hidden_size, seq_length, learning_rate,'\n",
    "                     'batch_size, vocab_size,'\n",
    "                    'num_epochs, num_layers, keep_prob')\n",
    "\n",
    "\n",
    "# Using the hyper parameters also used by:\n",
    "# Martin Gorner\n",
    "#https://github.com/martin-gorner/tensorflow-rnn-shakespeare/blob/master/rnn_train.py\n",
    "\n",
    "hps = hparams(hidden_size=512,\n",
    "              seq_length=30,\n",
    "              learning_rate=1e-3,\n",
    "              batch_size=200,\n",
    "              vocab_size=vocab_size,\n",
    "              num_epochs=50,\n",
    "              num_layers=3,\n",
    "              keep_prob=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class babble(object):\n",
    "    def __init__(self, hps, mode, debug=True):\n",
    "        self.hps=hps\n",
    "        self.mode=mode\n",
    "        self.batch_pointer=None\n",
    "    \n",
    "        \n",
    "    def buildGraph(self, variant=\"fixed_length\"):\n",
    "        # Extracting structural specifics from HPS\n",
    "        D = self.hps.vocab_size\n",
    "        H = self.hps.hidden_size\n",
    "        N = self.hps.batch_size\n",
    "        T = self.hps.seq_length\n",
    "\n",
    "        # Placeholder\n",
    "        with tf.name_scope(\"PlaceHolders\"):\n",
    "            self.X = tf.placeholder(tf.int32, [None, None], \"Inputs\")\n",
    "            self.Y = tf.placeholder(tf.int32, [None, None], \"Expected_Output\")\n",
    "            #is_training = tf.placeholder(tf.bool)\n",
    "            self.h0 = tf.placeholder(tf.float32, [None, self.hps.num_layers*self.hps.hidden_size], \"initial_hidden_state\")\n",
    "        \n",
    "        # No projection to embedding is performed in this experiment\n",
    "        # Inputs are simply translated to one hot\n",
    "        inputs = tf.one_hot(self.X,depth=self.hps.vocab_size)\n",
    "            \n",
    "        #with tf.name_scope(\"batch_norm\"):\n",
    "        #    inputs = tf.layers.batch_normalization(inputs)\n",
    "        cell = tf.contrib.rnn.GRUCell(H)\n",
    "        \n",
    "    \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.hps.keep_prob)\n",
    "        \n",
    "\n",
    "        multi_cell = tf.contrib.rnn.MultiRNNCell([cell]*self.hps.num_layers, state_is_tuple=False)\n",
    "\n",
    "        input_shape = tf.shape(self.X)\n",
    "        \n",
    "        #states = multi_cell.zero_state(self.hps.batch_size, tf.float32)\n",
    "        self.zerostate = multi_cell.zero_state(input_shape[0], dtype=tf.float32) \n",
    "        outputs, self.hidden_state = tf.nn.dynamic_rnn(\n",
    "                                         cell=multi_cell,\n",
    "                                         dtype=tf.float32,\n",
    "                                         inputs=inputs,\n",
    "                                         initial_state=self.h0\n",
    "                     \n",
    "        )\n",
    "\n",
    "        \n",
    "             \n",
    "        with tf.name_scope(\"Dense_Output_Layer\"):\n",
    "            outputs=tf.reshape(outputs, [-1, H])\n",
    "            scores=tf.layers.dense(outputs, D)\n",
    "        tf.summary.histogram('scores', scores)\n",
    "        self.scores=scores\n",
    "        return self.scores\n",
    "      \n",
    "        \n",
    "    def trainStep(self, scores):\n",
    "        # Extracting structural specifics from HPS\n",
    "        D = self.hps.vocab_size\n",
    "        H = self.hps.hidden_size\n",
    "        N = self.hps.batch_size\n",
    "        T = self.hps.seq_length             \n",
    "        \n",
    "        # Define the additional part of the network Used for training\n",
    "        # Loss and Optimizer\n",
    "        y_int=tf.reshape(self.Y, [-1])\n",
    "        with tf.name_scope(\"COST\"):\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.one_hot(y_int,self.hps.vocab_size),\n",
    "                logits=scores,\n",
    "                name=\"softMaxCrossEntropy\"\n",
    "            )\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        tf.summary.scalar('loss_', loss)\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"Predictions\"):\n",
    "            predictions = tf.cast(tf.argmax(scores, axis=-1, name=\"predictions\"), tf.int32)\n",
    "        #accuracy=tf.reduce_mean(tf.cast(tf.equal(predictions, self.Y), tf.float32))\n",
    "                                         \n",
    "        solver = tf.train.AdamOptimizer(self.hps.learning_rate)\n",
    "        #solver = tf.train.GradientDescentOptimizer(hps.learning_rate)\n",
    "        #solver = tf.train.MomentumOptimizer(hps.learning_rate, 0.9)\n",
    "        \n",
    "        tvars  = tf.trainable_variables()\n",
    "        gs_int = tf.gradients(loss, tvars)\n",
    "        grads = list(zip(gs_int, tf.trainable_variables()))\n",
    "        gs, _  = tf.clip_by_global_norm(gs_int, 3.0)\n",
    "        train_step = solver.apply_gradients(zip(gs,tvars), global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "        summary = tf.summary.merge_all()\n",
    "        return train_step, loss, summary, self.zerostate\n",
    "   \n",
    "    def train(self,sess, data,train_ops, writer=None, print_every=10):\n",
    "        itr=0\n",
    "        num_batches=data.shape[0]//self.hps.batch_size//self.hps.seq_length\n",
    "\n",
    "        for e in tqdm(list(range(hps.num_epochs)), desc='epoch'):\n",
    "            \n",
    "            total_correct=0\n",
    "            for i in range(num_batches):\n",
    "                itr+=1\n",
    "                x,y = self.getNextBatch(data)\n",
    "                feed_dict={self.X: x, self.Y:y, self.h0:np.zeros((self.hps.batch_size, self.hps.hidden_size*self.hps.num_layers))}\n",
    "                _, loss, summary,_ = sess.run(train_ops, feed_dict=feed_dict)\n",
    "                if writer is not None:\n",
    "                    writer.add_summary(summary)\n",
    "            #Sample after every 10 epochs to see how we are doing\n",
    "            if (e%10 == 0): \n",
    "                self.sample(sess,self.scores)\n",
    "\n",
    "        \n",
    "    \n",
    "    def sample(self, sess, scores, \n",
    "               seed=\"I have something to say\",\n",
    "               length=200, beam_width=5):\n",
    "        pd = tf.nn.softmax(scores)\n",
    "        x= [char_to_ix[i] for i in seed]\n",
    "        x=np.asarray(x).reshape(1,-1)\n",
    "        y=np.zeros_like(x)\n",
    "        feed_dict={self.X:x, self.Y:y, self.h0:np.zeros((1,self.hps.hidden_size*self.hps.num_layers))}\n",
    "        pred_str=[]\n",
    "        for i in range(length):\n",
    "            p,h0=sess.run([pd,self.hidden_state], feed_dict=feed_dict)\n",
    "            feed_dict[self.h0]=h0\n",
    "            p=p[-1]\n",
    "            #print(p.shape)\n",
    "            ix = np.random.choice(range(self.hps.vocab_size), p=p.ravel())\n",
    "            #ix=np.argmax(p.ravel())\n",
    "            p.reshape(1,-1)\n",
    "            feed_dict[self.X]=ix*np.ones((1,1))\n",
    "            pred_str.append(ix_to_chars[ix])\n",
    "        txt=''.join(pred_str)\n",
    "        print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # data is expected to be numpy array of indices\n",
    "    def getNextBatch(self, data):\n",
    "        if self.batch_pointer is None:\n",
    "            segment=data.shape[0]//self.hps.batch_size\n",
    "            self.batch_pointer = np.array([offset*segment for offset in range(self.hps.batch_size)])\n",
    "        else:\n",
    "            self.batch_pointer += 1\n",
    "            self.batch_pointer %= data.shape[0]\n",
    "        \n",
    "        \n",
    "        x=np.zeros((self.hps.batch_size, self.hps.seq_length))\n",
    "        y=np.zeros((self.hps.batch_size, self.hps.seq_length))\n",
    "        \n",
    "        indices=self.batch_pointer\n",
    "        \n",
    "        for i in range(self.hps.seq_length):\n",
    "            x[:,i]=np.take(data,indices, mode='wrap')\n",
    "            y[:,i]=np.take(data,indices+1, mode='wrap')\n",
    "            indices+=1\n",
    "            \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model=babble (hps,'train')\n",
    "sample=model.getNextBatch(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 10/50 [11:29<46:20, 69.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " \n",
      "\tAs makes it was your works] -Somethy, or bonessing that? why, like;\n",
      "\tAnd bord with memaforret's life upont, where being\n",
      "\tIs thought has too,, and have like our oping man eitile fortly\n",
      "Roppose of his \n",
      "----\n",
      "----\n",
      " \n",
      "\tI starn my griegy. I am a gain trijun, thou wilt not call him ill, then now with gried to time, whose enemy is still affections to ency drowning,\n",
      "\tThough he's as wantons of their accidents, fill at  \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 20/50 [23:06<34:36, 69.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "  'of rain,\n",
      "\tBe gain'd, and level, if it were and grossly thyself against a man; and hang this grief, go you up in his uncle By yours: of thy mapes, the foreod!\n",
      "\tIn while all I lived, rather thou one.\n",
      " \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 30/50 [34:37<23:02, 69.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "  'Then Wherefore? who is thoughts? Confounds thy bode,\n",
      "\tThis man losse so, my Mowbray, stande with thee than she as have I took thy niest how a grief. Look you for honey himself\n",
      "\tso wrangled him: his  \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 40/50 [46:17<11:45, 70.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      " ,\n",
      "\tAre in unrunkind trift of deity's. Good Petitionus: to that terrible, for the kmight were no more now she baits to taken,\n",
      "\tAnd that you would take love that not exercises, which we atandiff: even s \n",
      "----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 50/50 [58:39<00:00, 74.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----\n",
      "  I would not get him.\n",
      "\n",
      "SILVIUS\tHe etis flesh apart, folly and worth of Richmond neared; nothing admiting to no more right that with the promesses the loss of all the third,\n",
      "\tAs sixped with the relief, \n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "scores=model.buildGraph()\n",
    "train_ops=model.trainStep(scores)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)\n",
    "summaries=tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\n",
    "            os.path.join('./tf_logs', time.strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model.train(sess, data, train_ops,writer)\n",
    "    model.sample(sess, scores) \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
