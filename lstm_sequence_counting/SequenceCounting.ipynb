{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSTM to Count 1's in a sequence\n",
    "\n",
    "\n",
    "In the following example we explore Long Short Term Memory Network's abilty to count one's in a sequence. I came across this problem in http://monik.in blogpost about RNN. And stretched it a lttle further to see what kind of learning curve we can see for different sequence length.\n",
    "\n",
    "It's 'Many to One' topology from Unreasonable Effectiveness of Recurrent Neural Networks. The sequence length was kept constant in all the runs. A few other interesting experiments for explorations (planned):\n",
    "\n",
    "* Counting number of one's for valriable sequence length Using ideas of bucketting\n",
    "* Using external memory load method https://arxiv.org/abs/1410.5401 (Alex Graves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generating Training Data\n",
    "seq_length=20\n",
    "train_input = ['{0:0b}'.format(i).zfill(seq_length) for i in range(2**seq_length)]\n",
    "shuffle(train_input)\n",
    "train_input = [list(map(int,i)) for i in train_input]\n",
    "\n",
    "train_input = np.array(train_input)\n",
    "train_input = np.expand_dims(train_input, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generating training output\n",
    "train_output = np.sum(train_input, axis=1)\n",
    "\n",
    "# Let's make it one hot :) seq_length+1 bits\n",
    "train_output=(train_output == np.arange(seq_length+1)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "NUM_TRAIN= 10000\n",
    "test_input = train_input[NUM_TRAIN:]\n",
    "test_output = train_output[NUM_TRAIN:]\n",
    "\n",
    "train_input = train_input[:NUM_TRAIN]\n",
    "train_output = train_output[:NUM_TRAIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# HyperParameters of the model\n",
    "# Defining hyperparameter tuple and setting hyper parameters\n",
    "hparams = namedtuple('hyper_parameters', \n",
    "                     'hidden_size, seq_length, learning_rate,'\n",
    "                     'batch_size,'\n",
    "                    'num_epochs')\n",
    "\n",
    "\n",
    "hps = hparams(hidden_size=25,\n",
    "              seq_length=seq_length,\n",
    "              learning_rate=1e-3,\n",
    "              batch_size=1000,\n",
    "              num_epochs=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhavs/ENVS/vqa/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Training_Accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the network\n",
    "\n",
    "tf.reset_default_graph()\n",
    "data   = tf.placeholder(tf.float32, [None, seq_length, 1])\n",
    "target = tf.placeholder(tf.float32, [None, seq_length+1])\n",
    "\n",
    "num_hidden = hps.hidden_size\n",
    "with tf.name_scope(\"RNN\"):\n",
    "    cell=tf.contrib.rnn.BasicLSTMCell(num_hidden, state_is_tuple=True)\n",
    "    val, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Since we are going to take output from the last unrolled state\n",
    "# we transpose it to swap the batch dimension with \n",
    "# the num_unrolling dimension\n",
    "val = tf.transpose(val, [1,0,2])\n",
    "last = tf.gather(val, int(val.get_shape()[0])-1)\n",
    "\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "with tf.name_scope(\"DenseLayer\"):\n",
    "    logits = tf.layers.dense(last,int(target.get_shape()[1]), kernel_initializer=initializer)\n",
    "\n",
    "\n",
    "    \n",
    "prediction = tf.nn.softmax(logits)\n",
    "cross_entropy = -tf.reduce_sum(\n",
    "    target * tf.log(tf.clip_by_value(prediction, 1e-10, 1.0)))\n",
    "tf.summary.scalar(\"Cross_Entropy\", cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "train_op = optimizer.minimize(cross_entropy)\n",
    "errors = tf.not_equal(tf.argmax(target, 1), tf.argmax(prediction,1))\n",
    "accuracy = 1 - tf.reduce_mean(tf.cast(errors, tf.float32))\n",
    "tf.summary.scalar(\"Training_Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Configuration to ensure that tensorflow flow does NOT reserve all available memory on your GPU unnecessarily\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)\n",
    "sess.run(init_op)\n",
    "\n",
    "# Creating Merged Summary op\n",
    "summaries=tf.summary.merge_all()\n",
    "\n",
    "# Writer object will be passed to summary writers\n",
    "writer = tf.summary.FileWriter(\n",
    "            os.path.join('./tf_logs', time.strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=hps.batch_size\n",
    "n_batches=train_input.shape[0]//batch_size\n",
    "epoch=hps.num_epochs\n",
    "for i in tqdm(list(range(epoch)), desc='epoch'):\n",
    "    ptr=0\n",
    "    #for j in tqdm(list(range(n_batches)), desc='batches'):\n",
    "    for j in range(n_batches):\n",
    "        _, summary = sess.run([train_op, summaries] , feed_dict={\n",
    "            data:train_input[ptr:ptr+batch_size],\n",
    "            target: train_output[ptr:ptr+batch_size]\n",
    "        })\n",
    "        writer.add_summary(summary)\n",
    "        ptr+=batch_size\n",
    "    acc=sess.run(accuracy, feed_dict={\n",
    "        data: test_input,\n",
    "        target: test_output\n",
    "    })\n",
    "    print('Epoch: {:2d} Test Accuracy: {:3.1f}%'.format(i+1, 100*acc))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments with seq 16, 18 and 20: Any guess which cost profile belongs to which :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text](crossEntropy1.png \"'Loss'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
